<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Less Is More: Proxy Datasets in NAS Approaches">
  <meta property="og:title" content="Less Is More: Proxy Datasets in NAS Approaches"/>
  <meta property="og:description" content="CVPR 2022 Workshop"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Less Is More: Proxy Datasets in NAS Approaches">
  <meta name="twitter:description" content="CVPR 2022 Workshop">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Less Is More: Proxy Datasets in NAS Approaches</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://brian-moser.github.io/" target="_blank">Brian B. Moser</a>,</span>
                <span class="author-block">
                    <a href="https://scholar.google.de/citations?user=Jx9-sU0AAAAJ&hl=en" target="_blank">Federico Raue</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=MQV1N2sAAAAJ&hl=en" target="_blank">JÃ¶rn Hees</a>,
                  </span>
              <span class="author-block">
                    <a href="https://agd.cs.uni-kl.de/" target="_blank">Andreas Dengel</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">German Research Center for Artificial Intelligence<br>RPTU Kaiserslautern-Landau<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2022</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/CVPR2022W/NAS/html/Moser_Less_Is_More_Proxy_Datasets_in_NAS_Approaches_CVPRW_2022_paper.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Neural Architecture Search (NAS) defines the design of Neural Networks as a search problem. Unfortunately, NAS is computationally intensive because of various possibilities depending on the number of elements in the design and the possible connections between them. In this work, we extensively analyze the role of the dataset size based on several sampling approaches for reducing the dataset size (unsupervised and supervised cases) as an agnostic approach to reduce search time. We compared these techniques with four common NAS approaches in NAS-Bench-201 in roughly 1,400 experiments on CIFAR-100. One of our surprising findings is that in most cases we can reduce the amount of training data to 25 %, consequently also reducing search time to 25 %, while at the same time maintaining the same accuracy as if training on the full dataset. In addition, some designs derived from subsets out-perform designs derived from the full dataset by up to 22 p.p. accuracy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<p>
  In recent years, a new area of study called Neural Architecture Search (NAS) has emerged. NAS focuses on automating the creation of neural networks instead of relying on manual designs crafted by human researchers with their expertise. For instance, Google's AmoebaNet, a NAS approach, has achieved top-tier results in tasks like ImageNet Classification.
</p>
<p>
However, a significant drawback to NAS techniques is that they demand a considerable amount of computational time, especially when dealing with large datasets. Additionally, even after identifying a network structure, optimizing its weights remains a crucial step in assessing the quality of the design choices. AmoebaNet, for example, employs a trial-and-error evolutionary approach to select different configurations, necessitating training for each configuration to evaluate its performance. Consequently, researchers often limit the range of options explored by NAS algorithms to strike a balance between speed and effectiveness.
  
</p>
<p>
However, NAS techniques often utilize datasets that may not be ideal for the entire NAS process. Specifically, not every sample in the dataset contributes positively, and in some cases, they can even hinder overall performance. This phenomenon is particularly noticeable in datasets commonly used for Image Classification tasks, such as ImageNet.
</p><p>
To address this issue, we have undertaken a study to explore how the size of the training dataset can be leveraged to reduce the time required for NAS. In this research, we assess various sampling methods for selecting a subset of a dataset, considering both supervised and unsupervised scenarios. Our investigation involves four NAS approaches from NAS-Bench-201.
</p><p>
For our evaluation, we employed CIFAR-100 as our dataset. As a baseline, the NAS approach known as DARTS achieved a top-1 accuracy of 53.75% with a search time of 54 hours using an RTX 2080 GPU by NVIDIA. Remarkably, we were able to achieve a top-1 accuracy of 75.20% within a significantly reduced search time of just 13 hours by utilizing only 25% of the training data with the same NAS approach.
</p><p>
Furthermore, when applying this approach to another NAS technique, GDAS, we found that it was possible to derive an architecture with performance comparable to the baseline while using only 50% of the training data.
</p>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@InProceedings{Moser_2022_CVPR,
    author    = {Moser, Brian and Raue, Federico and Hees, J\"orn and Dengel, Andreas},
    title     = {Less Is More: Proxy Datasets in NAS Approaches},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {1953-1961}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
